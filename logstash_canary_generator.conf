input {
   java_generator {
        count     => 0 # Set to specific number of events to emit or 0 for until container stopped
        eps       => 1 # number of events per second to emit acceps decimal fractions. 0.033 = every 30 seconds.
        threads   => 1 # number of separate threads emitting log events.
        message   => "FOO CANARY" # a simple message to emit. See the ruby filter below for more interesting options.
   }
}

filter {
# Rename fields from input for avro schema
  mutate {
    rename => {
      "host"    => "hostname"
      "message" => "payload"
    }
  }


# Add necessary fields for avro schema
  mutate {
    add_field => {
      "payload_format" => "application/json"
      "target"         => ["intersting-metadata", "canary"]
      "datacenter"     => "top-secret"
      "owner"          => "owner_mailer@example.com"
      "loglevel"       => "INFO"
      "[envelope_fields][source_topic]"    => "${KAFKA_OUTPUT_TOPIC_ID:kafka_output_topic_id_not_set}"
      "[envelope_fields][source_brokers]"  => "${KAFKA_OUTPUT_BOOTSTRAP_SERVERS:kafka_output_bootstrap_servers_not_set}"
      "[envelope_fields][type]"           => "canary"
      "[envelope_fields][source]"         => "generator"
    }
  }

  mutate {
    replace => {
      "payload" => '{"@timestamp": "%{@timestamp}", "datacenter": "%{datacenter}", "hostname": "%{hostname}", "source_topic": "%{[envelope_fields][source_topic]}", "source_brokers": "%{[envelope_fields][source_brokers]}", "message": "%{payload}"}'
      }
  }

# ruby get the topics list - it has a grep for '^logs_|^metrics_' topic names.
# this has to be in init for the first event, then the main code updates @CANARY_TOPICS every 5minutes-ish.
# I set it to % 5 <=1 so that if we are producing canaries at > 1m there's a bit of overlap between 4-6 minute marks that the condition should still evaluate true.
ruby {
  init => '
    require "kafka"

    kafka = Kafka.new(ENV["KAFKA_OUTPUT_BOOTSTRAP_SERVERS"].split(","),
    ssl_ca_certs_from_system: true,
    sasl_scram_mechanism: "sha512",
    sasl_scram_username: ENV["KAFKA_OUTPUT_SCRAM_USERNAME"],
    sasl_scram_password: ENV["KAFKA_OUTPUT_SCRAM_PASSWORD"]
    )

    @CANARY_TOPICS = kafka.topics.grep(/^logs_|^metrics_/)
  '

  # Set an @metadata field with the topics array from the brokers
  code => '
    require "kafka"

    kafka = Kafka.new(ENV["KAFKA_OUTPUT_BOOTSTRAP_SERVERS"].split(","),
    ssl_ca_certs_from_system: true,
    sasl_scram_mechanism: "sha512",
    sasl_scram_username: ENV["KAFKA_OUTPUT_SCRAM_USERNAME"],
    sasl_scram_password: ENV["KAFKA_OUTPUT_SCRAM_PASSWORD"]
    )

    @topics_list_minutes = Time.now.min

    if (@topics_list_minutes % 5 <= 1)
      @CANARY_TOPICS = kafka.topics.grep(/^logs_|^metrics_/)
      event.set("[envelope_fields][topics_list_updated]", @CANARY_TOPICS.join(", "))
    end

    event.set("[@metadata][CANARY_TOPICS]", @CANARY_TOPICS)
  '
}

# Split the event into multiple events on CANARY_TOPICS array with a new metadata field CANARY_DESTINATION_TOPIC specific to each event
 split {
   field => "[@metadata][CANARY_TOPICS]"
   target => "[@metadata][CANARY_DESTINATION_TOPIC]"
 }

# Updating the envelope_fields.source_topic field reflecting the CANARY_DESTINATION_TOPIC
  mutate {
    replace => { "[envelope_fields][source_topic]" => "%{[@metadata][CANARY_DESTINATION_TOPIC]}" }
  }

}

output {

 kafka {

    topic_id                => "%{[@metadata][CANARY_DESTINATION_TOPIC]}"
    bootstrap_servers       => "${KAFKA_OUTPUT_BOOTSTRAP_SERVERS:kafka_output_bootstrap_servers_not_set}"
    id                      => "${KAFKA_OUTPUT_ID:kafka_output_id_not_set}"
    client_id               => "${KAFKA_OUTPUT_CLIENT_ID:kafka_output_client_id_not_set}"
    compression_type        => "gzip"
    batch_size              => "15000"
    linger_ms               => "100"
    buffer_memory           => "67108864"
    retries                 => 2

    key_serializer          => "org.apache.kafka.common.serialization.ByteArraySerializer"
    value_serializer        => "org.apache.kafka.common.serialization.ByteArraySerializer"

    sasl_mechanism          => "SCRAM-SHA-512"
    security_protocol       => "SASL_SSL"
    sasl_jaas_config        => "org.apache.kafka.common.security.scram.ScramLoginModule required username='${KAFKA_OUTPUT_SCRAM_USERNAME:no_scram_username_provided}' password='${KAFKA_OUTPUT_SCRAM_PASSWORD:no_scram_password_provided}';"

    codec => avro_schema_registry {
      endpoint              => "${KAFKA_OUTPUT_SCHEMA_URL:https://schema-registry.example.com}"
      subject_name          => "LogEventSchema"
      schema_version        => "${KAFKA_OUTPUT_SCHEMA_VERSION:1}"
    }
  }
}
